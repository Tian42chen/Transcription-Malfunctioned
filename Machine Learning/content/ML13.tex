\newpage
\section{半监督学习}
\subsection{未标记样本}
半监督学习：让学习器不依赖外界交互、自动地利用未标记样本来提升学习性能。

\begin{itemize}
    \item 纯(pure)半监督学习：假定训练数据中的未标记样本并非待预测
    的数据
    \item 直推学习：假定学习过程中所考虑的未标记样本恰是待预测数
    据，学习的目的就是在这些未标记样本上获得最优泛化性能
\end{itemize}

\subsection{生成式方法}
生成式方法(generative methods)是直接基于生成式模型的方法.此类方法
假设所有数据(无论是否有标记)都是由同一个潜在的模型“生成”的. 然后使用诸如极大似然或EM的方法估计参数. 

模型假设必须准确，即假设的生成式模型必须与真实数据分布吻
合；否则利用未标记数据反倒会降低泛化性能。


\subsection{半监督SVM}
TSVM：尝试将每个未标记样本分别作为正例或反例，然后在所
有这些结果中，寻求一个在所有样本(包括有标记样本和进行了标
记指派的未标记样本)上间隔最大化的划分超平面. 

在对未标记样本进行标记指派及调整的过程中，有可能出现类别
不平衡问题，即某类的样本远多于另一类，这将对SVM的训练造
成困扰。

\subsection{图半监督学习}
给定一个数据集，可将其映射为一个图，数据集中每个样本对应
于图中一个结点，若两个样本之间的相似度很高(或相关性很强)，
则对应的结点之间存在一条边，边的“强度”正比于样本之间的相
似度(或相关性)。


\subsection{基于分歧的方法}
多视图数据：一个数据对象往往同时拥有多个"属性集" (attribute set) ，每个属性集就构成了一个“视图”(view)。

\begin{itemize}
    \item 相容性：不同视图所包含的关于输出空间的信息是一致的。
    \item 相容互补性：假设数据拥有两个充分且条件独立视图，“充分”是
    指每个视图都包含足以产生最优学习器的信息；“条件独立”则是
    指在给定类别标记条件下两个视图独立。
\end{itemize}

在此情形下，可用一个简单的办法来利用未标记数据：
\begin{enumerate}
    \item 在每个视图上基于有标记样本分别训练出一个分类器
    \item 让每个分类器分别去挑选自己"最有把握的"未标记样本
    赋予伪标记，并将伪标记样本提供给另一个分类器作为新增
    的有标记样本用于训练更新...
    \item 这个"互相学习、共同进步"的过程不断迭代进行，直到两个分
    类器都不再发生变化，或达到预先设定的迭代轮数为止. 
\end{enumerate}

\subsection{半监督聚类}
聚类任务中获得的监督信息："必连"(must-link) 与"勿连" (cannot-link) 约束

约束k 均 值 (Constrained k - means) 算 法 是利用第一 类 监 督 信 息 的 代 表. 