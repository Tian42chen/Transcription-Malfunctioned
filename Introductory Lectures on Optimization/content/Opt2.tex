\newpage
\section{Foundations of Smooth Optimization}

\subsection{Relaxation and Approximation}
\subsubsection{Concepts of Relaxation and Approximation}
We call the sequence $\{a_k \}_{k=0}^\infty$ a \textbf{relaxation sequence} if $\forall k\ge 0$,
\begin{align*}
    a_{k+1}\le a_k
\end{align*}

Consider several methods for solving the following unconstrained minimization problem
\begin{align}
    \min_{\bm x\in \mathbb{R}^n}f(\bm x)\label{c21}
\end{align}
where $f(\bm x)$ is a smooth function. In order to do so, we generate a relaxation sequence
\begin{align*}
    \{ f(\bm x) \}_{k=0}^\infty,\ f(\bm x_{k+1})\le f(\bm x_k),\ k=0,1,\dots
\end{align*}

This strategy has the following important advantages:
\begin{enumerate}
    \item If $f(\bm x)$ is bounded below on $\mathbb{R}^n$, then the sequence $\{ f(\bm x_k) \}_{k=0}^\infty$ converges. 
    \item In any case we improve the initial value of the objective function. 
\end{enumerate}
In general, to \textbf{approximate} means to replace an initial complex object by a simplified one, which is close by its properties to the original.

In nonlinear optimization we usually apply local approximations based on derivatives(导数) of non-linear functions. These are the first order and the second order approximations (or, the linear and quadratic approximations). 

\subsubsection{First Order Approximation}
Let $f(\bm x)$ be differentiable at $\bar {\bm x}$. Then for $\bm y\in \mathbb{R}^n$, we have
\begin{align*}
    f(\bm y)=f(\bar {\bm x})+\braket{\nabla f(\bar{\bm x},\bm y-\bar{\bm x})}+o(\| \bm y-\bar{\bm x} \|)
\end{align*}
where $o(r)$(peano, 皮亚诺余项) is some function of $r\ge 0$, such that
\begin{align*}
    \lim_{r\to 0}\frac{1}{r}o(r)=0,\ o(0)=0
\end{align*}
In the sequel we fix the notation $\|\cdot \|$ for the standard Euclidean norm on $\mathbb{R}^n$: 
\begin{align*}
    \|\bm x\|=\left[ \sum_{i=1}^n\left( \bm x^{(i)} \right)^2 \right]^{1/2}
\end{align*}
The linear function $f(\bar {\bm x})+\braket{\nabla f(\bar{\bm x},\bm y-\bar{\bm x})}$ is called the linear approximation of $f$ at $\bar{\bm x}$. 

% $R->R, R^n->R, R^n->R^n$ 

\begin{itemize}
    \item The vector $\nabla f(\bm x)$ is called the \textbf{gradient} of function $f$ at $\bm x$. 
    \subitem Considering the points $\bm y_i=\bar{\bm x}+\epsilon e_i$, where $e_i$ is the $i$-th coordinate vector in $\mathbb{R}^n$, and taking the limit in $\epsilon\to 0$, we obtain the following coordinate representation of the gradient
    \begin{align*}
        \nabla f(\bm x)=\left( \frac{\partial f(\bm x)}{\partial \bm x^{(1)}},\dots,\frac{\partial f(\bm x)}{\partial \bm x^{(n)}} \right)^\top
    \end{align*}
    \item Denote by $\mathcal{L}_f(\alpha)$ the \textbf{level set}(等高线) of $f(\bm x)$:
    \begin{align*}
        \mathcal{L}_f(\alpha)=\{ \bm x\in \mathbb{R}^n|f(\bm x)\le \alpha \}
    \end{align*}
    \item Consider the \textit{set} of directions that are \textbf{tangent} to $\mathcal{L}_f(f(\bar{\bm x}))$ at $\bar{\bm x}$
    \begin{align*}
        S_f(\bar{\bm x})=\left\{ \bm s\in \mathbb{R}^n \left| \lim_{\bm y_k\to \bar{\bm x},\ f(\bm y_k)=f(\bar{\bm x})}=\frac{\bm y_k-\bar{\bm x}}{\|\bm y_k-\bar{\bm x}\|} \right. \right\}
    \end{align*}
\end{itemize}

\begin{lemma}
    If $\bm s\in S_f(\bar{\bm x})$, then $\braket{\nabla f(\bar{\bm x}),\bm s}=0$
\end{lemma}
\begin{proof}
    For $f(\bm y_k)=f(\bar{\bm x})$, we have,
    \begin{align*}
        f(\bm y_k)&=f(\bar{\bm x})+\braket{\nabla f(\bar{\bm x}), \bm y_k-\bar{\bm x}}+o(\| \bm y_k-\bar{\bm x} \|)\\
        &=f(\bar{\bm x})
    \end{align*}
    Therefore, $\braket{\nabla f(\bar{\bm x}), \bm y_k-\bar{\bm x}}+o(\| \bm y_k-\bar{\bm x} \|)=0$. Dividing this equation by $\| \bm y_k-\bar{\bm x} \|$, and taking the limit $ \bm y_k\to\bar{\bm x} $, we obtain
    \begin{align*}
        \braket{\nabla f(\bar{\bm x}),\lim_{\bm y_k\to \bar{\bm x}} \frac{\bm y_k-\bar{\bm x}}{\|\bm y_k-\bar{\bm x}\|}}=\braket{\nabla f(\bar{\bm x}), \bm s}=0
    \end{align*}
\end{proof}

% \paragraph{Fastest Local Decrease}
The directions $-\nabla f(\bar{\bm x})$ (the \textbf{antigradient}) is the directions of the \textbf{fastest local decrease} of $f(\bm x)$ at point $\bar{\bm x}$. (证明: 梯度负方向下降最快)

\begin{proof}
    Let $\bm s$ be a direction in $\mathbb{R}^n$, $\|\bm s\|=1$. Consider the local decrease of $f(\bm x)$ along $s$:
    \begin{align*}
        \Delta(\bm s)=\lim_{\alpha\to 0}\frac{1}{\alpha}[f(\bar{\bm x}+\alpha\bm s)-f(\bar{\bm x})]
    \end{align*}
    Note that $f(\bar{\bm x}+\alpha\bm s)-f(\bar{\bm x})=\alpha\braket{\nabla f(\bar{\bm x}),\bm s}+o(\alpha\|\bm s\|)$. Therefore, we have
    \begin{align*}
        \Delta(\bm s)=\braket{\nabla f(\bar{\bm x}),\bm s}
    \end{align*}
    By leveraging the Cauchy-Schwartz inequality, that is $-\|x\|\cdot\|y\|\le \braket{x,y}\le \|x\|\cdot\|y\|$, we obtain,
    \begin{align*}
        \Delta (\bm s)=\braket{\nabla f(\bar{\bm x}),\bm s}\ge -\| \nabla f(\bar{\bm x}) \|
    \end{align*}
    For the lower bound $\bar{\bm s}=-\frac{\nabla f(\bar{\bm x})}{\|\nabla f(\bar{\bm x})\|}$ (取到下界), we have
    \begin{align*}
        \Delta(\bar{\bm s})=- \frac{\braket{\nabla f(\bar{\bm x}),\nabla f(\bar{\bm x})}}{\|\nabla f(\bar{\bm x})\|}=-\|\nabla f(\bar{\bm x})\|
    \end{align*}
\end{proof}

% \paragraph{First-order Optimality Condition}

\begin{theorem}[First-order Optimality Condition]\label{c2T2}
    Let $\bm x^*$ be a local minimum of differentiable function $f(\bm x)$. Then $\nabla f(\bm x^*)=0$. 
\end{theorem}

\begin{proof}
    Since $\bm x^*$ is a local minimum of $f(\bm x)$, then there exists $r>0$ such that $\forall \bm y$, $\|\bm y-\bm x^*\|\le r$, we have $f(\bm y)\ge f(\bm x^*)$. Since $f$ is differentiable, this implies that 
    \begin{align*}
        f(\bm y)=f(\bm x^*)+\braket{\nabla f(\bm x^*), \bm y-\bm x^*}+o(\|\bm y-\bm x^*\|)\ge f(\bm x^*)
    \end{align*}
    Thus, $\forall \bm s, \|\bm s\|=1$, we have $\braket{\nabla f(\bm x^*), \bm s}\ge 0$. Consider the directions $\bm s$ and $-\bm s$, we get
    \begin{align*}
        \braket{\nabla f(\bm x^*),\bm s}=0, \forall \bm s, \|\bm s\|=1
    \end{align*}
    Finally, choosing $\bm s=e_i,i=1\dots n$, where $e_i$ is the $i$-th coordinate vector in $\mathbb{R}^n$, we obtain $\nabla f(\bm x^*)=0$. 
\end{proof}
仅必要条件, 反过来是stationary points (静态点).

e.g $f(x)=x^3. x\in\mathbb{R}^1$ at $x=0$. 

% \paragraph{Useful Corollary}
\begin{corollary}
    Let $\bm x^*$ be a local minimum of a differentiable function $f(\bm x)$ s.t. linear equality constraints
    \begin{align*}
        \bm x\in\mathcal{L}\equiv\{ \bm x \in \mathbb{R}^n | A\bm x=\bm b \}\ne \emptyset
    \end{align*}
    where $A$ is an $m\times n$ matrix and $\bm b\in \mathbb{R}^m, m<n$. Then there exists a vector of multipliers $\lambda^*$ such that
    \begin{align}
        \nabla f(\bm x^*)=A^\top\lambda^* \label{c22}
    \end{align}
\end{corollary}
$\mathcal{L}$ is the Null space of matrix $A$. 
\begin{proof}
    Consider some vector $\bm u_i,i=1\dots k$, that form a basis of the Null space (零空间) of matrix $A$. Then any $\bm x\in\mathcal{L}$ can be represented as follows:
    \begin{align*}
        \bm x=\bm x(\bm y)\equiv \bm x^*+\sum_{i=1}^k\bm y^{(i)}\bm u_i, \bm y\in\mathbb{R}^k
    \end{align*}
    Moreover, the point $\bm y=0$ is a local minimum of the function $\phi(\bm y)=f(\bm x(\bm y))$. In view of \textbf{Theorem} \ref{c2T2}, $\nabla \phi(0)=0$. This means that 
    \begin{align*}
        \pard{\phi(0)}{\bm y^{(i)}}=\pard{\phi(0)}{\bm x(\bm y)}\cdot\pard{\bm x(\bm y)}{\bm y^{(i)}}=\braket{\nabla f(\bm x^*), \bm u_i}=0,i=1\dots k
    \end{align*}
    and \ref{c22} follows. (因为零空间与行空间正交)
\end{proof}

\subsubsection{Second Order Approximation}
Let function $f(\bm x)$ be twice differentiable at $\bar{\bm x}$. Then 
\begin{align*}
    f(\bm y)=&f(\bar{\bm x})+\braket{\nabla f(\bar{\bm x}),\bm y-\bar{\bm x}}\\
    &+\frac{1}{2}\braket{\nabla^2 f(\bar{\bm x})(\bm y-\bar{\bm x}),\bm y-\bar{\bm x}}+o(\|\bm y-\bar{\bm x}\|^2)
\end{align*}

The quadratic function
\begin{align*}
    f(\bar{\bm x})+\braket{\nabla f(\bar{\bm x}),\bm y-\bar{\bm x}}+\frac{1}{2}\braket{\nabla^2 f(\bar{\bm x})(\bm y-\bar{\bm x}),\bm y-\bar{\bm x}}
\end{align*}
is called the \textbf{quadratic} (or \textbf{second order}) approximation of function $f$ at $\bar{\bm x}$. 

Recall that the $(n\times n)$ matrix $\nabla^2 f(\bm x)$ has the following entries:
\begin{align*}
    (\nabla^2 f(\bm x))^{(i,j)}=\pard{^2f(\bm x)}{\bm x^{(i)}\partial \bm x^{(j)}}
\end{align*}
This matrix is called \textbf{Hessian} of function $f$ at $\bm x$. 

Note that the Hessian is a \textbf{symmetric} matrix:
\begin{align*}
    \nabla^2 f(\bm x)=[\nabla^2 f(\bm x)]^\top
\end{align*}

Remark: 
\begin{itemize}
    \item Notation $A\succeq 0$ means that $A$ is \textbf{positive semidefinite}(半正定): 
    \begin{align*}
        \bm x^\top A \bm x\ge 0\ \forall \bm x \in \mathbb{R}^n
    \end{align*}
    \item Notation $A\succ 0$ means that $A$ is \textbf{positive definite}(正定): 
    \begin{align*}
        \bm x^\top A \bm x> 0\ \forall \bm x \in \mathbb{R}^n
    \end{align*}
\end{itemize}

\begin{theorem}
    Let $\bm x^*$ be a local minimum of a twice differentiable function $f(\bm x)$. Then 
    \begin{align*}
        \nabla f(\bm x^*)=0,\ \nabla^2 f(\bm x^*)\succeq 0
    \end{align*}
\end{theorem}
\begin{proof}
    Since $\bm x^*$ is a local minimum of function $f(\bm x)$, there exists $r>0$ such that $\forall \bm y, \|\bm y-\bm x^*\|\le r$, we have
    \begin{align*}
        f(\bm y)\ge f(\bm x^*)
    \end{align*}
    In view of \textbf{Theorem} \ref{c2T2}, $\nabla f(\bm x^*)=0$. 

    Therefore, for any such $\bm y$,
    \begin{align*}
        f(\bm y)&=f(\bm x^*)+\frac{1}{2}\braket{\nabla^2 f(\bm x^*)(\bm y-\bm x^*),\bm y-\bm x^*}+o(\|\bm y-\bm x^*\|^2)\\
        &\ge f(\bm x^*)
    \end{align*}
    两项都除以 $\|\bm y-\bm x^*\|^2$, let $\bm y\to \bm x^*$, and let $\bm s=\frac{\bm y-\bm x^*}{\|\bm y-\bm x^*\|}$
    \begin{align*}
        \lim_{\bm y\to \bm x^*}\left(\frac{1}{2}\frac{\braket{\nabla^2 f(\bm x^*)(\bm y-\bm x^*),\bm y-\bm x^*}}{\|\bm y-\bm x^*\|^2}+\frac{o(\|\bm y-\bm x^*\|^2)}{\|\bm y-\bm x^*\|^2}\right)&\ge 0\\
        \braket{\nabla^2 f(\bm x^*)\bm s,\bm s}&\ge 0
    \end{align*}
    $\forall \bm s, \|\bm s\|=1$
\end{proof}
\begin{theorem}
    Let function $f(\bm x)$ be twice differentiable on $\mathbb{R}^n$ and let $\bm x^*$ satisfy the following conditions:
    \begin{align*}
        \nabla f(\bm x^*)=0,\ \nabla^2 f(\bm x^*)\succ 0
    \end{align*}
    Then $\bm x^*$ is a strict local minimum of $f(\bm x)$. 
\end{theorem}

Remark: A point $\bar{\bm x}\in \mathbb{R}^n$ is an unconstrained strict local minimum of a function $f:\mathbb{R}^n \to \mathbb{R}$ if $\exists \epsilon >0$ such that $f(\bar{\bm x})<f(\bm x)$ $\forall \bm x\in B(\bar{\bm x}, \epsilon), \bm x\ne \bar{\bm x}$, where $B(\bar{\bm x}, \epsilon):=\{ \bm x|\|\bm x-\bar{\bm x}\|\le \epsilon \}$
%TODO 21
% \begin{proof}
    
% \end{proof}

$\lambda_1(A)$ 表示 $A$ 的最小特征值, $\lambda_{max}(A)$表示最大的特征值. 


\subsection{Classes of differentiable function}
\subsubsection{Class \texorpdfstring{$C_L^{k,p}(\mathbb{R}^n)$}. }
Consider a classes of \textbf{differentiable} functions which meet a \textbf{Lipschitz conditon} for a derivative of certain order.

Let $Q\subseteq \mathbb{R}^n$. We denote by $C_L^{k,p}(Q)$ the class of functions with the following properties:
\begin{itemize}
    \item Any $f\in C_L^{k,p}(Q)$ is $k$ times continuously \textbf{differentiable} on $Q$.
    \item Its $p$-th derivative is \textbf{Lipschitz continous} on $Q$ with the constant $L$:
    \begin{align*}
        \left\| f^{(p)}(\bm x)-f^{(p)}(\bm y) \right\|\le L\|\bm x-\bm y\|
    \end{align*}
    $\forall x,y,\in Q$. 
\end{itemize}

Clearly, we always have
\begin{enumerate}
    \item $p\le k$ 显然成立
    \item If $q\ge k$, then $C_L^{q,p}(Q)\subseteq C_L^{k,p}$. 
    \item Note also that these classes possess the following property:
    \subitem If $f_1\in C_{L_1}^{k,p}(Q), f_2\in C_{L_1}^{k,p}(Q)$ and $\alpha, \beta\in \mathbb{R}^1$, then for 
    \begin{align*}
        L_3=|\alpha| L_1+|\beta|L_2
    \end{align*}
    we have $\alpha f_1+\beta f_2\in C_{L_3}^{k,p}(Q)$. 
\end{enumerate}
Remark. We use notation $f\in C^k(Q)$ for a function $f$ which is $k$ times continuously differ entiable on $Q$.

\subsubsection{Class \texorpdfstring{$C_L^{1,1}(\mathbb{R}^n)$}. } 
Consider $C_L^{1,1}(\mathbb{R}^n)$, the class of functions with \textbf{Lipschitz continous gradient}. By definition, the inclusion $f\in C_L^{1,1}(\mathbb{R}^n)$ implies that $\forall \bm x,\bm y\in \mathbb{R}^n$, 
\begin{align*}
    \| \nabla f(\bm x)-\nabla f(\bm y) \|\le L\|\bm x-\bm y\| %\label{c23}
\end{align*}
Let us give a sufficient condition for that inclusion. 

\begin{lemma}
    A function $f(\bm x)$ belongs to $C_L^{2,1}(\mathbb{R}^n)\subset C_L^{1,1}(\mathbb{R}^n)$, if and only if 
    \begin{align*}
        \|\nabla^2f(\bm x) \|\le L,\ \forall \bm x\in \mathbb{R}^n %\label{c24}
    \end{align*}
\end{lemma}
%TODO P28-31
矩阵二范数

$g(a)=\nabla f(x+a(y-x))$, $g(0)=\nabla f(x), g(1)=\nabla f(y)$

$g(1)=g(0)+\int_0^1 g'(\tau )d\tau$

$g'(\tau)=\frac{\partial \nabla f(x+\tau(y-x))}{\partial (x+\tau(y-x))}\frac{\partial (x+\tau(y-x))}{\partial \tau}=\nabla^2 f(x+\tau(y-x))(y-x)$

e.g. %TODO 32-33

\paragraph{geometric interpretation}
The next statement is important for the geometric interpretation of function from $C_L^{1,1}(\mathbb{R}^n)$. 

\begin{lemma}
    Let $f\in C_L^{1,1}(\mathbb{R}^n)$. Then $\forall \bm x, \bm y\in \mathbb{R}^n$, we have
    \begin{align*}
        |f(\bm y)-f(\bm x)-\braket{\nabla f(\bm x), \bm y-\bm x}|\le \frac{L}{2}\|\bm y-\bm x\|^2 %\label{c26}
    \end{align*}
    $f$ is called smooth. 
\end{lemma}
Remark. \\
$f(\bm y)$ 和其一阶逼近 $g(\bm y)=f(\bm x)+\braket{\nabla f(\bm x), \bm y-\bm x}$ 的距离的上界. 

%TODO 35-37

%TODO 37-38
强凸

\subsubsection{Class \texorpdfstring{$C_{M}^{2,1}(\mathbb{R}^n)$}. }
Consider class $C_M^{2,2}(\mathbb{R}^n)$. That is $\forall \bm x,\bm y\in \mathbb{R}^n$, we have
\begin{align*}
    \|\nabla^2f(\bm x)-\nabla^2 f(\bm y)\|\le M\|\bm x-\bm y\|
\end{align*}

\begin{lemma}
    Let $f\in C_M^{2,2}(\mathbb{R}^n)$. Then for any $\bm x,\bm y\in \mathbb{R}^n$, we have 
    \begin{align*}
        \|\nabla f(\bm y)-\nabla f(\bm x)-\nabla^2f(\bm x)(\bm y-\bm x)\| \le \frac{M}{2}\|\bm y-\bm x\|^2 %\label{c28}
    \end{align*}
\end{lemma}
%TODO 40-41

\begin{corollary}
    Let $f\in C_M^{2,2}(\mathbb{R}^n)$ adn $\|\bm y-\bm x\|=r$. Then 
    \begin{align*}
        \nabla^2f(\bm x)-Mr I_n \succeq \nabla ^2f(\bm y)\succeq \nabla^2f(\bm x)+Mr I_n
    \end{align*}
    where $I_n$ is the unit matrix $\mathbb{R}^n$. 
\end{corollary}

%TODO 42-44

对于矩阵 $A$ 和 $B$, $A \succeq B \iff A-B\succeq 0$. 

\subsection{Convex Function}
\subsubsection{Definition of The Convex Function and The Class \texorpdfstring{$\mathcal{F}^k$}. }
Consider the unconstrained minimization problem
\begin{align}
    \min_{\bm x\in \mathbb{R}^n}f(\bm x)\label{c39}
\end{align}
where the objective function $f(\bm x)$ is smooth enough

定义 smooth 足够的问题, 但这个约束很弱, 在局部最小不能保证收敛, 不能保证上下界. 

增加几个假设 %TODO assumption 块
\begin{assumption}
    For any $f\in \FC$, first-order optimality condition is \textbf{sufficient} for a point to be a \textbf{global solution} to \ref{c39}
\end{assumption}

The possibility to verify the inclusion $f\in\FC$ in a simple way.

\begin{assumption}
    If $f_1,f_2\in \FC$ and $\alpha, \beta \ge 0$, then $\alpha f_1+\beta f_2\in \FC$. 
\end{assumption}
\begin{assumption}
    Any linear function $f(\bm x)=\alpha +\braket{a,\bm x}$ belongs to $\FC$. 
\end{assumption}
\paragraph{Definition of The Convex Function} Consider
%TODO 7-8
\begin{align*}
    f(\bm y)\ge f(\bm x_0)+\braket{\nabla f(\bm x_0), \bm y-\bm x_0}
\end{align*}
函数大于线性逼近. 

\begin{definition}[Convex Set]
    A set $\mathcal{Q}\subseteq \mathbb{R}^n$ is called \textbf{convex} if $\forall \bm x,\bm y\in \mathcal{Q}$, and $\alpha \in [0,1]$, we have
    \begin{align*}
        \alpha \bm x+(1-\alpha \bm y)\in \mathcal{Q}
    \end{align*}
\end{definition}
(集合中任取两点, 连线都是属于集合的, $\mathcal{F}$ 表示是凸的)

We denote by $\FC^k(\mathcal{Q})$ the class we discussed above, and call it \textbf{class of the convex function}:
\begin{itemize}
    \item Any $f\in\FC^k(\mathcal{Q})$ is a \textbf{convex function}
    \item Any $f\in\FC^k(\mathcal{Q})$ is $k$ times continuously \textbf{differentiable} on $\mathcal{Q}$
    \item We assume $\mathcal{Q}=\R^n$ in this chapter
\end{itemize}

\begin{definition}[Convex Function]
    A continuously differentiable function $f(\cdot)$ is called \textbf{convex} on a convex set $\mathcal{Q}$ (notation $f\in \mathcal{F}^1(\mathcal{Q}^n)$) if $\forall \bm x,\bm y\in \mathcal{Q}$ we have
    \begin{align*}
        f(\bm y)\ge f(\bm x)+\braket{\nabla f(\bm x),\bm y-\bm x}
    \end{align*}
    If $-f(\bm x)$ is convex, we call $f(\bm x)$ \textbf{concave}. 
\end{definition}

\subsubsection{Properties of The Convex Function}

\begin{theorem}[Global Property]
    If $f\in\FC^1(\R^n)$ and $\nabla(f\bx^*)=0$, then $\bx^*$ is the global minimum of $f(\bx)$ on $\R^n$
\end{theorem}
%TODO 10

\begin{lemma}[Conic Combination]
    If $f_1$ and $f_2$ belong to $\FC^1(\R^n)$, and $\alpha, \beta \ge 0$, then the function $f=\alpha f_1+\beta f_2$ also belongs to $\FC^1(\R^n)$. 
\end{lemma}
%TODO 11

\begin{lemma}[Affine Composition]
    If $f\in\FC^1(\R^n)$， $\bm b\in\R^m$, and $A:\R^n\to\R^m$, then 
    \begin{align*}
        \phi(\bx)=f(A\bx+\bm b)\in\FC^1(\R^m)
    \end{align*}
\end{lemma}
%TODO 12

\begin{lemma}[Pointwise maximum and supremum]\quad

    If $f_i(\bx), i\in I$, are convex, then 
    \begin{align*}
        g(\bx)=\max_{i\in I}f_i(\bx)
    \end{align*}
    is also convex. 
\end{lemma}

\begin{lemma}[Convex monotone composition]\quad

    \begin{enumerate}
        \item If $f$ is a convex function on $\R^n$ and $F(\cdot)$ is a convex and non-decreasing function on $\R$, then $g(\bx)=F(f(\bx))$ is convex. 
        \item If $f_i,i=1,\dots,m$ are convex functions on $\R^n$ and $F(\by_1,\dots,\by_m)$ is convex and non-decreasing (component-wise) in each argument, then 
        \begin{align*}
            g(\bx)=F(f_1(\bx),\dots,f_m(\bx))
        \end{align*}
        is convex. 
    \end{enumerate}
\end{lemma}

\begin{lemma}[Partial minimization]
    If $f(\bx,\by)$ is convex in $(\bx,\by)\in\R^n$ and $Y$ is a convex set, then 
    \begin{align*}
        g(\bm x)=\inf_{\by\in Y}f(\bx,\by)
    \end{align*}
    is convex. 
\end{lemma}

%TODO lemma 20-22 自己证

\subsubsection{Equivalent Definitions}

\begin{theorem}
    A continuously \textbf{differentiable} function $f$ belongs to the class $\FC^1(\R^n)$ if and only if $\forall \bx,\by\in\R^n$ and $\alpha\in [0,1]$ we have
    \begin{align*}
        f(\alpha \bx+(1-\alpha )\by)\le \alpha f(\bx)+(1-\alpha)f(\by)
    \end{align*}
\end{theorem}
(凸组合的函数$\le$ 函数的凸组合)%TODO 方向导数定义补充 16-17

\begin{theorem}
    A continuously differentiable function $f$ belongs to the class $\FC^1(\R^n)$ if and only if $\forall \bx,\by\in \R^n$, we have
    \begin{align*}
        \braket{\nabla f(\bx)-\nabla f(\by), \bx-\by}\ge 0
    \end{align*}
\end{theorem}

%TODO 泰勒逼近的积分形余项 18-19

\begin{theorem}
    A twice continuously  differentiable function $f$ belongs to class $\FC^2(\R^n)$ if and only if $\forall x\in \R^n$, we have
    \begin{align*}
        \nabla^2f(\bx)\succeq 0
    \end{align*}
\end{theorem}

\subsubsection{Examples}
\begin{enumerate}
    \item 
\end{enumerate}



\subsection{Smooth and Convex Function}
\subsubsection{The Class \texorpdfstring{$\FC_L^{k,l}(\R^n)$}.}
\begin{enumerate}
    \item Any function $f\in \FC_L^{k,l}(\R^n)$ is convex
    \item the meaning of the index is the same as $C_L^{k,l}(\R^n)$
    \item The most important class of that type is $\FC_L^{1,1}(\R^n)$, the class of convex functions with \textbf{Lipschitz continous gradient}. 
\end{enumerate}

\subsubsection{Necessary and Sufficient Conditions for The Class \texorpdfstring{$\FC_L^{1,1}(\R^n)$}.}

\begin{theorem}
    All the conditions below, holding $\forall \bx,\by\in\R^n$ and $\alpha\in[0,1]$, are equivalent to the inclusion $f\in\FC_L^{1,1}(\R^n)$:
    \begin{equation}
        0\le f(\by)-f(\bm x)-\braket{\nabla f(\bm x),\by-\bx}\le \frac{L}{2}\norm{\bx-\by}^2
    \end{equation}
    \begin{equation}
        f(\bx)+\braket{\nabla f(\bm x),\by-\bx}+\frac{1}{2L}\norm{\nabla f(\bx)-\nabla f(\by)}^2\le f(\by) 
    \end{equation}
    \begin{equation}
        \frac{1}{L}\norm{\nabla f(\bx)-\nabla f(\by)}^2\le \braket{\nabla f(\bx)-\nabla f(\by), \bx-\by}
    \end{equation}
    \begin{equation}
        0\le  \braket{\nabla f(\bx)-\nabla f(\by), \bx-\by} \le L\norm{\bx-\by}^2
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \alpha f(\bx)+(1-\alpha)f(\by)\ge& f(\alpha \bx+(1-\alpha)\by)\\
            &+\frac{\alpha(1-\alpha)}{2L}\norm{\nabla f(\bx)-\nabla f(\by)}^2 
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            0&\le \alpha f(\bx)+(1-\alpha)f(\by)- f(\alpha \bx+(1-\alpha)\by)\\
            &\le \alpha(1-\alpha)\frac{L}{2}\norm{\bx-\by}^2
        \end{aligned}
    \end{equation}
\end{theorem}

%TODO 一个证明图, 可以使用自动机的 tikz 实现

%TODO 差 (20), (19) 证回去

\subsubsection{Necessary and Sufficient Conditions for The Class \texorpdfstring{$\FC_L^{2,1}(\R^n)$}. }

\begin{theorem}
    Twice continuously differentiable function $f\in \FC_L^{2,1}(\R^n)$ if and only if $\forall \bx \in \R^n$, we have
    \begin{align*}
        0\preceq \nabla^2 f(\bx)\preceq LI_n
    \end{align*}
\end{theorem}

\subsection{Strongly Convex Function}
\subsubsection{Definition of The Strongly Convex and The Class \texorpdfstring{$\mathcal{S}_{\mu}^1(\mathbb{R}^n)$}.}

\begin{definition}
    A continuously differentiable function $f(\bm x)$ is called \textbf{strongly convex} on $\R^n$ (notation $f\in\SC_\mu^1(\R^n)$) if there exists a constant $\mu>0$ such that $\forall \bx,\by\in\R^n$, we have
    \begin{align*}
        f(\bm y)\ge f(\bm x) +\braket{\nabla f(\bm x), \bm y-\bm x} +\frac{1}{2}\mu \|\bm y-\bm x\|^2
    \end{align*}
    $\mu$ is called \textbf{convexity parameter}(凸参数) of $f$. 
\end{definition}

\subsubsection{Property of Strongly Convex Function}

\begin{theorem}
    If $f\in \mathcal{S}_\mu^1(\mathbb{R}^n)$ and $\nabla f(\bm x^*)=0$, then 
    \begin{align*}
        f(\bm x)\ge f(\bm x^*)+\frac{1}{2}\mu\|\bm x-\bm x^*\|^2,\ \forall \bx\in\R^n
    \end{align*}
\end{theorem}

\begin{lemma}
    If $f_1\in \mathcal{S}_{\mu_1}^1(\mathbb{R}^n), f_2 \in \mathcal{S}_{\mu_2}^1(\mathbb{R}^n)$ and $\alpha,\beta\ge 0$ then,
    \begin{align*}
        f=\alpha f_1+\beta f_2\in \mathcal{S}_{\alpha \mu_1+\beta \mu _2}^1(\mathbb{R}^n)
    \end{align*}
\end{lemma}

加线性函数增加凸性, 让多解变为单解. 

\begin{theorem}
    If $f\in \mathcal{S}_\mu^1(\mathbb{R}^n)$, then $\forall \bm x,\bm y\in \mathbb{R}^n$, we have
    \begin{equation}
        f(\bm y)\le f(\bm x)+ \braket{f(\bm x),\bm y-\bm x} +\frac{1}{2\mu}\|\nabla f(\bm x)-\nabla f(\bm y)\|^2
    \end{equation}
    \begin{equation}
        \braket{\nabla f(\bx)-\nabla f(\by),\bx-\by}\le \frac{1}{\mu}\norm{\nabla f(\bx)-\nabla f(\by)}^2
    \end{equation}
\end{theorem}

$\phi(\nu)$

\subsubsection{Equivalent Definitions}

\begin{theorem}
    Let $f$ be continuously differentiable. Both cnoditions below, holding $\forall \bm x,\bm y\in\mathbb{R}^n$ and $\alpha \in [0,1]$, are equivalent to inclusion $\mathcal{S}_\mu^1(\mathbb{R}^n)$
    \begin{equation}
        \braket{\nabla f(\bx)-\nabla f(\by),\bx-\by}\ge\mu\norm{\bx-\by}^2
    \end{equation}
    \begin{equation}
        \alpha f(\bx)+(1-\alpha)f(\by)\ge f(\alpha \bx+(1-\alpha)\by)
        +\alpha(1-\alpha)\frac{\mu}{2}\norm{\bx-\by}^2
    \end{equation}
\end{theorem}
%证明可以在教科书上找到

$\SC_\mu^2(\R^n)\subseteq \SC_\mu^1(\R^n)$
\begin{theorem}
    Two times continuously differentiable functions $f$ belongs to the class $\SC_\mu^2(\R^n)$ if and only if $\bx\in\R^n$
    \begin{align*}
        \nabla^2 f(\bm x)\succeq \mu I_n
    \end{align*}
\end{theorem}
$\bm s\in \mathbb{R}^n$

$\braket{I_n \bm s, \bm s}$

\subsubsection{Examples}
\begin{enumerate}
    \item 
\end{enumerate}

\subsection{Smooth and Strongly Convex Function}
\subsubsection{The Class \texorpdfstring{$\mathcal{S}_{\mu,L}^{1,1}(\mathbb{R}^n)$}. }

The value $Q_f=\frac{L}{\mu}\ge 1$ is called the \textbf{condition number}(条件数) of function $f$. 

% SVD 相关

\subsubsection{Property of Smooth and Strongly Convex Function}

\begin{theorem}
    If $f\in \mathcal{S}_{\mu,L}^{1,1}(\mathbb{R}^n)$, then $\forall \bm x,\bm y\in \mathbb{R}^n$, we have
    \begin{align*}
        \braket{\nabla f(\bm x)-\nabla f(\bm y), \bm x-\bm y}&\ge \frac{\mu L}{\mu+L}\|\bm x-\bm y\|^2\\
        &+\frac{1}{\mu+L}\|\nabla f(\bm x)-\nabla f(\bm y)\|^2
    \end{align*}
\end{theorem}
退化后和 ... 与 ... 相关 %TODO \mu \to 0, L\to \infty

\subsection{Conclusion}
\subsubsection{Upper Bounds on Functional Components}
Lipschitz Continuity: $\forall \bx,\by\in\R^n$,
\begin{itemize}
    \item Zerothorder Condition:
    \begin{align*}
        |f(\bx)-f(\by)|\le L\norm{\bx-\by}
    \end{align*}
    \item First-order Condition:
    \begin{align*}
        \norm{\nabla f(\bx)-\nabla f(\by)}\le L\norm{\bx-\by}
    \end{align*}
    \item $p$-order Condition
    \begin{align*}
        \norm{\nabla^p f(\bx)-\nabla^p f(\by)}_*\le L\norm{\bx-\by},\ p\ge 2
    \end{align*}
\end{itemize}

\subsubsection{Lower Bounds on Functional Components}
Convexity: $\forall \bx,\by\in\R^n$, $\alpha\in[0,1]$,
\begin{itemize}
    \item Zerothorder Condition:
    \begin{align*}
        \alpha f(\bx)+(1-\alpha)f(\by)-f(\alpha \bx+(1-\alpha)\by)\ge 0
    \end{align*}
    \item First-order Condition:
    \begin{align*}
        D_f(\bx,\bx)\triangleq f(\by)-\{ f(\bx)+ \braket{\nabla f(\bx),\by-\bx} \}&\ge 0\\
        \braket{\nabla f(\bx)-\nabla f(\by),\bx-\by}  &\ge 0
    \end{align*}
    \item Second-order Condition:
    \begin{align*}
        \nabla^2 f(\bx)\succeq 0
    \end{align*}
\end{itemize}

Strong Convexity: $\forall \bx,\by\in\R^n$,
\begin{itemize}
    \item Zerothorder Condition:
    \begin{align*}
        \alpha f(\bx)+(1-\alpha)f(\by)-f(\alpha \bx+(1-\alpha)\by)\\
        \ge \alpha(1-\alpha)\frac{\mu}{2}\norm{\bx-\by}^2
    \end{align*}
    \item First-order Condition:
    \begin{align*}
        D_f(\bx,\by) &\ge \frac{\mu}{2}\norm{\bx-\by}^2\\
        \braket{\nabla f(\bx)-\nabla f(\by),\bx-\by}&\ge \mu\norm{\bx-\by}^2
    \end{align*}
    \item Second-order Condition
    \begin{align*}
        \nabla ^2 f(\bx)\succeq \mu I_n
    \end{align*}
\end{itemize}

\subsubsection{Other Lower Bounds on Functional Components}
\begin{itemize}
    \item Weak Strong Convexity(WSC) 
    \begin{align*}
        D_f(\bx^*,\by)\ge \frac{\mu}{2}\norm{\bx^*-\bx}^2,\ \forall \bx\in\R^n
    \end{align*}
    \item Restricted Secant Inequality(RSI)
    \begin{align*}
        \braket{\nabla f(\bx), \bx-\bx^*}\ge \mu\norm{\bx^*-\bx}^2,\ \forall \bx\in\R^n
    \end{align*}
    \item Polyak-Łojaciewicz(PL) 
    \begin{align*}
        \frac{1}{2}\norm{\nabla f(\bx)}^2\ge \mu (f(\bx)-f(\bx^*)),\ \forall \bx\in\R^n
    \end{align*}
    \item Error Bounds(EB)
    \begin{align*}
        \norm{\nabla f(\bx)}\ge \mu \norm{\bx^*-\bx},\ \forall \bx\in\R^n
    \end{align*}
    \item Quadratic Growth(QG)
    \begin{align*}
        f(\bx)-f(\bx^*)\ge \frac{\mu}{2}\norm{\bx^*-\bx},\ \forall \bx\in\R^n
    \end{align*}
\end{itemize}

(SC)$\to$(WSC)$\to$(RSI)$\to$(EB)$\equiv$(PL)$\to$(QG)

If $f$ is convex, (RSI)$\equiv$(EB)$\equiv$(PL)$\equiv$(QG)

\begin{theorem}
    $\mu$-strongly convex function $\to$ $\mu$-PL
\end{theorem}